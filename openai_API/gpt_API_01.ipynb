{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como usar la API de GPT\n",
    "\n",
    "docuementación: https://platform.openai.com/docs/api-reference/completions/create\n",
    "\n",
    "tokenizador: https://platform.openai.com/tokenizer\n",
    "\n",
    "\n",
    "1. Tener una cuenta de pago de openAI ---> NO ES LA MISMA QUE GPT4 son dos diferentes\n",
    "2. Desde la línea de comandos usar cualquiera de las siguientes opciones:\n",
    "- pip install openai\n",
    "- python -m pip install openai\n",
    "3. Solicitar una clave de API la página\n",
    "4. Importar el modulo openai y setear o fijar nuestra clave de API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai                                                               # importamos \n",
    "import os\n",
    "\n",
    "openai.api_key = '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'     # fijamos la clave\n",
    "# openai.api_key = os.getenv(\"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\")\n",
    "\n",
    "\n",
    "\n",
    "#mi_clave_openAI = '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'\n",
    "# openai.api_key = os.getenv(mi_clave_openAI)\n",
    "# openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ------------\n",
      "absl-py                      2.0.0\n",
      "aiofiles                     23.2.1\n",
      "aiohttp                      3.9.0\n",
      "aiosignal                    1.3.1\n",
      "altair                       5.1.2\n",
      "annotated-types              0.6.0\n",
      "anyio                        3.7.1\n",
      "argon2-cffi                  23.1.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.3.0\n",
      "asttokens                    2.4.1\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.4\n",
      "async-timeout                4.0.3\n",
      "attrs                        23.1.0\n",
      "Babel                        2.13.1\n",
      "beautifulsoup4               4.12.2\n",
      "bleach                       6.1.0\n",
      "cachetools                   5.3.2\n",
      "certifi                      2023.11.17\n",
      "cffi                         1.16.0\n",
      "charset-normalizer           3.3.2\n",
      "click                        8.1.7\n",
      "colorama                     0.4.6\n",
      "comm                         0.2.0\n",
      "contourpy                    1.2.0\n",
      "cycler                       0.12.1\n",
      "dataclasses-json             0.6.2\n",
      "debugpy                      1.8.0\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "distro                       1.8.0\n",
      "exceptiongroup               1.1.3\n",
      "executing                    2.0.1\n",
      "fastapi                      0.104.1\n",
      "fastjsonschema               2.19.0\n",
      "ffmpy                        0.3.1\n",
      "filelock                     3.13.1\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.45.0\n",
      "fqdn                         1.5.1\n",
      "frozenlist                   1.4.0\n",
      "fsspec                       2023.10.0\n",
      "gast                         0.4.0\n",
      "google-auth                  2.23.4\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "gradio                       4.7.1\n",
      "gradio_client                0.7.0\n",
      "greenlet                     3.0.1\n",
      "grpcio                       1.59.3\n",
      "h11                          0.14.0\n",
      "h5py                         3.10.0\n",
      "httpcore                     1.0.2\n",
      "httpx                        0.25.1\n",
      "huggingface-hub              0.19.4\n",
      "idna                         3.4\n",
      "importlib-metadata           6.8.0\n",
      "importlib-resources          6.1.1\n",
      "ipykernel                    6.26.0\n",
      "ipython                      8.17.2\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.19.1\n",
      "Jinja2                       3.1.2\n",
      "joblib                       1.3.2\n",
      "json5                        0.9.14\n",
      "jsonpatch                    1.33\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.20.0\n",
      "jsonschema-specifications    2023.11.1\n",
      "jupyter_client               8.6.0\n",
      "jupyter_core                 5.5.0\n",
      "jupyter-events               0.9.0\n",
      "jupyter-lsp                  2.2.0\n",
      "jupyter_server               2.10.1\n",
      "jupyter_server_terminals     0.4.4\n",
      "jupyterlab                   4.0.9\n",
      "jupyterlab-pygments          0.2.2\n",
      "jupyterlab_server            2.25.2\n",
      "keras                        2.10.0\n",
      "Keras-Preprocessing          1.1.2\n",
      "kiwisolver                   1.4.5\n",
      "langchain                    0.0.340\n",
      "langsmith                    0.0.66\n",
      "libclang                     16.0.6\n",
      "Markdown                     3.5.1\n",
      "markdown-it-py               3.0.0\n",
      "MarkupSafe                   2.1.3\n",
      "marshmallow                  3.20.1\n",
      "matplotlib                   3.8.2\n",
      "matplotlib-inline            0.1.6\n",
      "mdurl                        0.1.2\n",
      "mistune                      3.0.2\n",
      "multidict                    6.0.4\n",
      "mypy-extensions              1.0.0\n",
      "nbclient                     0.9.0\n",
      "nbconvert                    7.11.0\n",
      "nbformat                     5.9.2\n",
      "nest-asyncio                 1.5.8\n",
      "networkx                     3.2.1\n",
      "notebook                     7.0.6\n",
      "notebook_shim                0.2.3\n",
      "numpy                        1.26.2\n",
      "oauthlib                     3.2.2\n",
      "openai                       1.3.4\n",
      "opt-einsum                   3.3.0\n",
      "orjson                       3.9.10\n",
      "overrides                    7.4.0\n",
      "packaging                    23.2\n",
      "pandas                       2.1.3\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "Pillow                       10.1.0\n",
      "pip                          23.3.1\n",
      "platformdirs                 4.0.0\n",
      "prometheus-client            0.19.0\n",
      "prompt-toolkit               3.0.41\n",
      "protobuf                     3.19.6\n",
      "psutil                       5.9.6\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.1\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "pydantic                     2.5.1\n",
      "pydantic_core                2.14.3\n",
      "pydub                        0.25.1\n",
      "Pygments                     2.17.1\n",
      "pyparsing                    3.1.1\n",
      "pypdf                        3.17.1\n",
      "python-dateutil              2.8.2\n",
      "python-json-logger           2.0.7\n",
      "python-multipart             0.0.6\n",
      "pytz                         2023.3.post1\n",
      "pywin32                      306\n",
      "pywinpty                     2.0.12\n",
      "PyYAML                       6.0.1\n",
      "pyzmq                        25.1.1\n",
      "referencing                  0.31.0\n",
      "regex                        2023.10.3\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rich                         13.7.0\n",
      "rpds-py                      0.13.1\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.3.2\n",
      "scipy                        1.11.4\n",
      "semantic-version             2.10.0\n",
      "Send2Trash                   1.8.2\n",
      "setuptools                   68.2.2\n",
      "shellingham                  1.5.4\n",
      "six                          1.16.0\n",
      "sniffio                      1.3.0\n",
      "soupsieve                    2.5\n",
      "SQLAlchemy                   2.0.23\n",
      "stack-data                   0.6.3\n",
      "starlette                    0.27.0\n",
      "tenacity                     8.2.3\n",
      "tensorboard                  2.10.1\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.10.0\n",
      "tensorflow-estimator         2.10.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.3.0\n",
      "terminado                    0.18.0\n",
      "threadpoolctl                3.2.0\n",
      "tiktoken                     0.5.1\n",
      "tinycss2                     1.2.1\n",
      "tomli                        2.0.1\n",
      "tomlkit                      0.12.0\n",
      "toolz                        0.12.0\n",
      "tornado                      6.3.3\n",
      "tqdm                         4.66.1\n",
      "traitlets                    5.13.0\n",
      "typer                        0.9.0\n",
      "types-python-dateutil        2.8.19.14\n",
      "typing_extensions            4.8.0\n",
      "typing-inspect               0.9.0\n",
      "tzdata                       2023.3\n",
      "uri-template                 1.3.0\n",
      "urllib3                      2.1.0\n",
      "uvicorn                      0.24.0.post1\n",
      "wcwidth                      0.2.11\n",
      "webcolors                    1.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.6.4\n",
      "websockets                   11.0.3\n",
      "Werkzeug                     3.0.1\n",
      "wheel                        0.41.3\n",
      "wrapt                        1.16.0\n",
      "yarl                         1.9.3\n",
      "zipp                         3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora podemos hacer una solicitud a la API de GPT\n",
    "\n",
    "MODELOS A USAR: \n",
    "\n",
    "- Para texto: \"gpt-4-1106-preview\"\n",
    "\n",
    "- Para visión: \"gpt-4-vision-preview\"\n",
    "\n",
    "- para texto en la completions API: \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "\n",
    "## Completions API \n",
    "\n",
    "### Token log probabilities\n",
    "\n",
    "The completions API can provide a limited number of log probabilities associated with the most likely tokens for each output token. This feature is controlled by using the logprobs field. This can be useful in some cases to assess the confidence of the model in its output.\n",
    "\n",
    "### Inserting text\n",
    "\n",
    "The completions endpoint also supports inserting text by providing a suffix in addition to the standard prompt which is treated as a prefix. This need naturally arises when writing long-form text, transitioning between paragraphs, following an outline, or guiding the model towards an ending. This also works on code, and can be used to insert in the middle of a function or file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formato de peticion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# response = client.completions.create(\n",
    "#   model=\"gpt-3.5-turbo-instruct\",\n",
    "#   prompt=\"Write a tagline for an ice cream shop.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formato de la respuesta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"choices\": [\n",
    "#     {\n",
    "#       \"finish_reason\": \"length\",\n",
    "#       \"index\": 0,\n",
    "#       \"logprobs\": null,\n",
    "#       \"text\": \"\\n\\n\\\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack\"\n",
    "#     }\n",
    "#   ],\n",
    "#   \"created\": 1683130927,\n",
    "#   \"id\": \"cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD\",\n",
    "#   \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "#   \"object\": \"text_completion\",\n",
    "#   \"usage\": {\n",
    "#     \"completion_tokens\": 16,\n",
    "#     \"prompt_tokens\": 10,\n",
    "#     \"total_tokens\": 26\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de uso\n",
    "\n",
    "Los argumentos que le pasamos a la petición **openai.completions.create**: \n",
    "\n",
    "Obligatorios:\n",
    "\n",
    "- **model**: El ID del modelo a utilizar.\n",
    "- **prompt**: El prompt inicial para el modelo.\n",
    "\n",
    "Opcionales:\n",
    "\n",
    "- **max_tokens**: El número máximo de tokens que se deben generar.\n",
    "- **temperature**: Ajusta la aleatoriedad de la respuesta.\n",
    "- **top_p**: Probabilidad acumulada para el muestreo de núcleos.\n",
    "- **n**: Número de respuestas independientes a generar.\n",
    "- **stream**: Si se debe devolver la respuesta como un flujo.\n",
    "- **logprobs**: El número de probabilidades logarítmicas a devolver.\n",
    "- **echo**: Si se debe incluir el prompt en la respuesta.\n",
    "- **stop**: Una secuencia donde el modelo debe dejar de generar más texto.\n",
    "- **presence_penalty**: Penalización por la presencia de tokens.\n",
    "- **frequency_penalty**: Penalización por la frecuencia de tokens.\n",
    "- **best_of**: Controla la generación de múltiples respuestas.\n",
    "- **logit_bias**: Sesgos para modificar la probabilidad de generación de tokens específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# respuesta = openai.Completion.create(\n",
    "#     model=\"text-davinci-002\",                                  # Modelo GPT-3\n",
    "#     prompt=\"Translate the following English text to French:\",  # Tu prompt\n",
    "#     max_tokens=60,                                             # Número máximo de tokens\n",
    "#     temperature=1,                                             # Aleatoriedad completa\n",
    "#     top_p=1,                                                   # Probabilidad acumulada completa\n",
    "#     n=1,                                                       # Una sola respuesta\n",
    "#     stream=False,                                              # Respuesta no en flujo\n",
    "#     logprobs=None,                                             # Sin logaritmos de probabilidades\n",
    "#     echo=False,                                                # No incluir el prompt en la respuesta\n",
    "#     stop=None,                                                 # Sin secuencia de detención\n",
    "#     presence_penalty=0,                                        # Sin penalización por presencia\n",
    "#     frequency_penalty=0,                                       # Sin penalización por frecuencia\n",
    "#     best_of=1,                                                 # Mejor de una generación\n",
    "#     logit_bias={}                                              # Sin sesgo de logits\n",
    "#     seed= None                                                 # reproducibilidad de los datos\n",
    "#     suffix= None                                               # sufijo para después de la completacion\n",
    "# )\n",
    "# print(respuesta.choices[0].text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Regularización L1 y L2\n",
      "- Pros: Ayuda a reducir el overfitting al penalizar los pesos más grandes en la red neuronal, lo que reduce la complejidad del modelo y evita que se ajuste demasiado a los datos de entrenamiento.\n",
      "- Contras: Puede llevar a un rendimiento deficiente si se aplica en exceso, ya que puede eliminar demasiada información importante de los datos.\n",
      "\n",
      "2. Dropout\n",
      "- Pros: Ayuda a reducir el overfitting al desactivar aleatoriamente un porcentaje de neuronas durante el entrenamiento, lo que evita que las neuronas se vuelvan demasiado dependientes de otras.\n",
      "- Contras: Puede llevar a una pérdida de información importante si se aplica en exceso, ya que algunas neuronas pueden ser desactivadas durante el entrenamiento y no contribuir al aprendizaje del modelo.\n",
      "\n",
      "3. Data Augmentation\n",
      "- Pros: Ayuda a reducir el overfitting al generar nuevas imágenes a partir de las existentes, lo que aumenta la cantidad de datos de entrenamiento y evita que el modelo se ajuste demasiado a los datos existentes.\n",
      "- Contras: Puede ser costoso computacionalmente y puede generar imágenes que no son realistas o relevantes para el problema en cuestión.\n",
      "\n",
      "4. Early Stopping\n",
      "- Pros: Ayuda a reducir el overfitting al detener el entrenamiento del modelo cuando el rendimiento en el conjunto de validación comienza a empeorar, lo que evita que el modelo se ajuste demasiado a los datos de entrenamiento.\n",
      "- Contras: Puede detener el entrenamiento demasiado pronto y no permitir que el modelo alcance su máximo rendimiento.\n",
      "\n",
      "5. Reducción de la complejidad del modelo\n",
      "- Pros: Ayuda a reducir el overfitting al simplificar la arquitectura del modelo, lo que reduce la cantidad de parámetros y evita que el modelo se ajuste demasiado a los datos de entrenamiento.\n",
      "- Contras: Puede llevar a un rendimiento deficiente si se reduce demasiado la complejidad del modelo y no puede capturar la complejidad de los datos.\n",
      "\n",
      "6. Validación cruzada\n",
      "- Pros: Ayuda a reducir el overfitting al evaluar el rendimiento del modelo en\n"
     ]
    }
   ],
   "source": [
    "# Hacemos una solicitud a la API de GPT-3 para generar un texto basado en un prompt que proporcionamos.\n",
    "\n",
    "def enviar_promt_completions_mode(\n",
    "        mi_prompt, \n",
    "        model= \"gpt-3.5-turbo-instruct\", \n",
    "        temp= 1, \n",
    "        max_tokens= 500, \n",
    "        probabilidad_acumulada= 0.2, \n",
    "        frequency_penalty= 0, \n",
    "        presence_penalty= 0):\n",
    "    \n",
    "    # completions es la seccion de openAI para completar texto\n",
    "    respuesta = openai.completions.create(\n",
    "        model= model,                           # indicamos el modelo que vamos a utlizar\n",
    "        prompt= mi_prompt,                      # el mensaje que le pasamos a chat \n",
    "        temperature= temp,                      # regula el grado de aleatoriedad de la respuesta 0 (siempre la misma respuesta), 1 (respuesta con aleatoriedad)\n",
    "        max_tokens= max_tokens,                 # el maximo de token que queremos generar por cada promt \n",
    "        top_p= probabilidad_acumulada,          # delimitamos el universo de tokens de los cuales puede elegir para responder 1 = analiza todos 0.1 solo el 10% con mayor probabilidad, etc\n",
    "\n",
    "        # se mueven en un rango de -2,2 \n",
    "        frequency_penalty=frequency_penalty,    # si repiten tokens recibe penalización  \n",
    "        presence_penalty= presence_penalty      # con que un token aparezca una vez ya recibe penalización\n",
    "    )\n",
    "\n",
    "    return respuesta.choices[0].text.strip()    # el indice donde esta la respuesta de nuestro modelo\n",
    "\n",
    "respuesta = enviar_promt_completions_mode('¿Que técnicas puedo utilizar para evitar el overfitting en el entrenamiento de una CNN?, haz una lista de pros y contras de cada una')\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'gpt-3.5-turbo-instruct'\n",
    "\n",
    "# respuesta = enviar_promt('Eres un humorista y me vas a dar consejo. La chica que me gusta estudia derecho, dame un chiste relacionado con abogados que sea bueno', \n",
    "#                          model= model, \n",
    "#                          temp= 1)\n",
    "# print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando imagenes\n",
    "\n",
    "DOCUMENTACIÓN: https://platform.openai.com/docs/api-reference/images/create?lang=python\n",
    "\n",
    "Podemos generar imágenes utilizando dall-e y la api openai.images.generate\n",
    "\n",
    "### Argumentos de openai.generate.image(): \n",
    "\n",
    "Obligatorios:\n",
    "\n",
    "- **model**: El modelo de generación de imágenes a utilizar.\n",
    "- **prompt**: La descripción o instrucción textual para generar la imagen.\n",
    "\n",
    "Opcionales:\n",
    "\n",
    "- **n**: El número de imágenes a generar.\n",
    "- **size**: Las dimensiones de la imagen generada (por ejemplo, \"1024x1024\").\n",
    "- **seed**: Una semilla para la generación determinística de la imagen, que asegura resultados consistentes con el mismo input.\n",
    "- **style**: puede ser **vivid** (para imagenes más realistas) o **natural** (imagenes menos realistas). SOLO EN DALL-E-3\n",
    "- **user**: identificador unico para end-users\n",
    "- **response_format**: **url** o **b64_json**\n",
    "- **quality**: la calidad de la imagen SOLO EN DALL-E-3 standard o hd.\n",
    "\n",
    "### formato de petición: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = openai.Image.create(\n",
    "\n",
    "#     model=\"modelo-de-imagen\",          # Model: El modelo de IA específico para la generación de imágenes.\n",
    "#     prompt=\"Una descripción textual\",  # Prompt: La descripción o instrucción textual para la imagen.\n",
    "#     n=1,                               # N: Número de imágenes a generar (por defecto es 1).\n",
    "#     size=\"1024x1024\",                  # Size: Las dimensiones de la imagen generada (valor por defecto \"1024x1024\").\n",
    "#     seed=None                          # Seed: Una semilla para la generación determinística (por defecto es None, lo que significa aleatorio).\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_imagen(descripcion:str, model:str= 'dall-e-3', tamaño:str= '1024x1024',n:int= 1, \n",
    "                   estilo:str= 'natural', calidad:str= 'standard', formato:str= 'url' ): \n",
    "    import openai\n",
    "    \n",
    "    if model=='dall-e-3':\n",
    "        url_imagen = openai.images.generate(\n",
    "            prompt=descripcion, \n",
    "            model= model, \n",
    "            size= tamaño,\n",
    "            response_format=formato, \n",
    "            style= estilo, \n",
    "            quality=calidad\n",
    "        )\n",
    "    else: \n",
    "        url_imagen = openai.images.generate(\n",
    "            prompt=descripcion, \n",
    "            model= model, \n",
    "            size= tamaño,\n",
    "            response_format=formato, \n",
    "        )\n",
    "    return url_imagen.data[0].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-zemGjGpqiIpOslczvHjiBqBS/user-uRdv532XLWTFtpEqufLwdm5l/img-DTuyHObBlo6JsIK2mHRsV8Vm.png?st=2023-11-22T14%3A02%3A36Z&se=2023-11-22T16%3A02%3A36Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-11-22T00%3A10%3A05Z&ske=2023-11-23T00%3A10%3A05Z&sks=b&skv=2021-08-06&sig=tPXyasOag0UD/LmTG9S427E7U2Ib5B%2BpEoHM98Bcflw%3D\n"
     ]
    }
   ],
   "source": [
    "descr_imagen = 'quiero unas montañas nevadas, con un lago, y la aurora boreal en el cielo, de noche'\n",
    "\n",
    "mi_imagen= generar_imagen(descripcion=descr_imagen)\n",
    "\n",
    "print(mi_imagen)\n",
    "\n",
    "# las urls tienen validez de 24 horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-zemGjGpqiIpOslczvHjiBqBS/user-uRdv532XLWTFtpEqufLwdm5l/img-fqawVdM2aWd4bnXYBDgv3lfo.png?st=2023-11-22T14%3A03%3A42Z&se=2023-11-22T16%3A03%3A42Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-11-21T20%3A18%3A54Z&ske=2023-11-22T20%3A18%3A54Z&sks=b&skv=2021-08-06&sig=5BOKqLp0IkrF/dXwpMMrA%2B8oqyxVCkiJ037rDdGeM4c%3D\n"
     ]
    }
   ],
   "source": [
    "mi_imagen= generar_imagen(descripcion=descr_imagen, calidad= 'hd')\n",
    "\n",
    "print(mi_imagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAT COMPLETIONS API\n",
    "\n",
    "Chat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it’s just as useful for single-turn tasks without any conversation.\n",
    "\n",
    "### Formato de solicitud: \n",
    "\n",
    "The main input is the messages parameter. Messages must be an array of message objects, where each object has a role (either \"system\", \"user\", or \"assistant\") and content. Conversations can be as short as one message or many back and forth turns.\n",
    "\n",
    "\n",
    "Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages.\n",
    "\n",
    "\n",
    "The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
    "\n",
    "\n",
    "The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
    "\n",
    "\n",
    "Including conversation history is important when user instructions refer to prior messages. In the example above, the user’s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model’s token limit, it will need to be shortened in some way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#   model=\"gpt-3.5-turbo\",\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "#   ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#   model=\"gpt-3.5-turbo-1106\",\n",
    "#   response_format={ \"type\": \"json_object\" },\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "#   ]\n",
    "# )\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formato de respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"choices\": [\n",
    "#     {\n",
    "#       \"finish_reason\": \"stop\",\n",
    "#       \"index\": 0,\n",
    "#       \"message\": {\n",
    "#         \"content\": \"The 2020 World Series was played in Texas at Globe Life Field in Arlington.\",\n",
    "#         \"role\": \"assistant\"\n",
    "#       }\n",
    "#     }\n",
    "#   ],\n",
    "#   \"created\": 1677664795,\n",
    "#   \"id\": \"chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW\",\n",
    "#   \"model\": \"gpt-3.5-turbo-0613\",\n",
    "#   \"object\": \"chat.completion\",\n",
    "#   \"usage\": {\n",
    "#     \"completion_tokens\": 17,\n",
    "#     \"prompt_tokens\": 57,\n",
    "#     \"total_tokens\": 74\n",
    "#   }\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para obtener la respuesta usar el siguiente formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de uso\n",
    "\n",
    "Los argumentos de la peticion **client.chat.completions.create**\n",
    "\n",
    "Obligatorios:\n",
    "\n",
    "- **model**: El modelo de GPT que deseas utilizar (por ejemplo, \"gpt-3.5-turbo\").\n",
    "- **messages**: Una lista de mensajes previos en la conversación. Cada mensaje es un diccionario con role (como \"user\" o \"system\") y content.\n",
    "\n",
    "Opcionales:\n",
    "\n",
    "- **max_tokens**: El número máximo de tokens a generar en la respuesta.\n",
    "- **temperature**: Controla la aleatoriedad de la respuesta 0 siempre la misma respuesta 1 probabilidad de diferente respuesta alta.\n",
    "- **top_p**: Controla la diversidad de la respuesta basándose en la probabilidad de los tokens, cercano a 1 considera una gama más amplia de posibles respuestas, cercano a 0.1 restringe las respuestas del modelo a aquellas que son más probables.\n",
    "- **logprobs**: El número de probabilidades de logaritmo a incluir con cada respuesta.\n",
    "- **echo**: Si se incluye el prompt en la respuesta.\n",
    "- **stop**: Una secuencia en el texto que indica al modelo que deje de generar texto.\n",
    "- **user**: ID del usuario para personalizar la experiencia.\n",
    "- **response_format**: Especifica el formato de la respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# respuesta_chat = client.chat_completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",                                                            # Modelo GPT para chat\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"Hello, who won the world cup in 2018?\"}],  # Mensajes anteriores\n",
    "#     max_tokens=60,                                                                    # Número máximo de tokens\n",
    "#     temperature=1,                                                                    # Aleatoriedad completa\n",
    "#     top_p=1,                                                                          # Probabilidad acumulada completa\n",
    "#     logprobs=None,                                                                    # Sin logaritmos de probabilidades\n",
    "#     echo=False,                                                                       # No incluir los mensajes anteriores en la respuesta\n",
    "#     stop=None,                                                                        # Sin secuencia de detención\n",
    "#     user=None                                                                         # Sin ID de \n",
    "#     response_format= None                                                             # sin un formato especifico por defecto\n",
    "# )\n",
    "# print(respuesta_chat.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enviar_promt_chat_completions_mode(\n",
    "        mensaje: list, modelo: str = \"gpt-4-1106-preview\", formato: dict = None, \n",
    "        maximo_tokens: int= 200, aleatoriedad: float= 0.5, probabilidad_acumulada: float= 0.5):\n",
    "     \n",
    "    respuesta = openai.chat.completions.create(\n",
    "        messages= mensaje, \n",
    "        model= modelo, \n",
    "        response_format= None, \n",
    "        max_tokens=maximo_tokens, \n",
    "        temperature=aleatoriedad, \n",
    "        top_p= probabilidad_acumulada\n",
    "    )\n",
    "\n",
    "    if formato == {'type': 'json_object'}: \n",
    "        return respuesta['choices'][0]['message']['content']\n",
    "    \n",
    "    else: return respuesta.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lógica difusa puede ser una herramienta valiosa cuando se integra con reinforcement learning (RL), especialmente en escenarios donde el feedback humano es parte del proceso de aprendizaje, como en el caso de los modelos de lenguaje natural. Aquí hay algunas formas en que la lógica difusa puede aplicarse en este contexto:\n",
      "\n",
      "1. **Interpretación de Feedback Humano**: El feedback humano es inherentemente impreciso y subjetivo. La lógica difusa puede ser utilizada para modelar y cuantificar este tipo de feedback. Por ejemplo, las respuestas humanas como \"un poco incorrecto\" o \"bastante bien\" pueden ser traducidas a valores difusos que pueden ser procesados por el sistema de RL.\n",
      "\n",
      "2. **Definición de Funciones de Recompensa**: En RL, la función de recompensa es crucial para guiar el aprendizaje del agente. Sin embargo, definir una función de recompensa clara y precisa puede ser desafiante en tareas de lenguaje natural. La lógica difusa permite crear funciones de recompensa que manejan la ambigüedad y la incertidumbre inherentes a la evaluación del lenguaje.\n",
      "\n",
      "3. **Políticas de Decisión Difusas**: Las políticas de decisión en RL, que determinan la acción a tomar en cada estado,\n"
     ]
    }
   ],
   "source": [
    "mi_prompt_0 = [\n",
    "    {'role': 'system', 'content': 'Eres un  profesor doctorado en inteligencia artificial con un profundo \\\n",
    "                                   conocimiento en deep learning, reinforcement learning, y lógica difusa'}, \n",
    "                                \n",
    "    {'role': 'user', 'content': '¿como puedo aplicar lógica difusa en el ámbito de reinforcement learning \\\n",
    "                                  with human feedback aplicado a modelos de lenguaje natural?'},\n",
    "\n",
    "#   {'role': 'assistant', 'content': 'esto se usa para decirle a chat gpt COMO debe responder'}\n",
    "]\n",
    "\n",
    "respuesta_0 = enviar_promt_chat_completions_mode(\n",
    "    mensaje=mi_prompt_0, \n",
    "    maximo_tokens=300, \n",
    "    probabilidad_acumulada=0.5, \n",
    "    aleatoriedad=0.5, \n",
    ")\n",
    "\n",
    "print(respuesta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
