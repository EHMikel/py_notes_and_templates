{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use functions with a knowledge base\n",
    "\n",
    "Este Notebook se basa en los conceptos del Notebook de argument generation, creando un agente con acceso a una base de conocimientos y dos funciones que puede llamar en función de los requisitos del usuario.\n",
    "\n",
    "Crearemos un agente que utilice datos de arXiv para responder a preguntas sobre temas académicos. Dispone de dos funciones\n",
    "\n",
    "- <span style=\"color: rgb(0,255,0);\">***get_articles***</span>: Una función que obtiene artículos de arXiv sobre un tema y los resume para el usuario con enlaces.\n",
    "- <span style=\"color: rgb(0,255,0);\">***read_article_and_summarize***</span>: Esta función toma uno de los artículos previamente buscados, lo lee en su totalidad y resume el argumento central, las pruebas y las conclusiones.\n",
    "\n",
    "Esto te hará sentir cómodo con un flujo de trabajo multifunción que puede elegir entre múltiples servicios, y donde algunos de los datos de la primera función se persisten para ser utilizados por la segunda.\n",
    "\n",
    "## Recorrido\n",
    "\n",
    "Esta guia le llevará a través del siguiente flujo de trabajo:\n",
    "\n",
    "- **Utilidades de búsqueda**: Creación de las dos funciones que acceden a arXiv en busca de respuestas.\n",
    "- **Configurar Agente**: Creación del comportamiento del Agente que evaluará la necesidad de una función y, si se requiere una, llamará a esa función y presentará los resultados de vuelta al agente.\n",
    "- **Conversación arXiv**: Reunir todo esto en una conversación en directo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy\n",
    "# !pip install tenacity\n",
    "# !pip install tiktoken==0.3.3\n",
    "# !pip install termcolor \n",
    "# !pip install openai\n",
    "# !pip install requests\n",
    "# !pip install arxiv\n",
    "# !pip install pandas\n",
    "# !pip install PyPDF2\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                 # modulo del sistema operativo\n",
    "\n",
    "import arxiv                              # Módulo para interactuar con el archivo de preprints de ArXiv.\n",
    "import ast                                # Módulo para procesar árboles de sintaxis abstracta.\n",
    "import concurrent                         # Módulo para ejecución concurrente.\n",
    "from csv import writer                    # Para escribir en archivos CSV.\n",
    "from IPython.display import display, Markdown, Latex    # Para mostrar datos en entornos Jupyter.\n",
    "import json                               # Para manejar datos en formato JSON.\n",
    "import openai                             # Módulo de OpenAI.\n",
    "import pandas as pd                       # Para manejo y análisis de datos (PD).\n",
    "from PyPDF2 import PdfReader              # Para leer archivos PDF.\n",
    "import requests                           # Para realizar solicitudes HTTP.\n",
    "from scipy import spatial                 # Para cálculos espaciales y de distancia.\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt     # Para reintentos con estrategias de espera.\n",
    "import tiktoken                           # No es un módulo estándar; podría estar relacionado con tokens o autenticación.\n",
    "from tqdm import tqdm                     # Para barras de progreso.\n",
    "from termcolor import colored             # Para colorear texto en la terminal.\n",
    "import config\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "api = config.OPENAI_API_KEY\n",
    "#  KEY ='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'\n",
    "openai.api_key = api "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilidades de búsqueda\n",
    "\n",
    "Primero configuraremos algunas utilidades que sustentarán nuestras dos funciones.\n",
    "\n",
    "Los artículos descargados se almacenarán en un directorio (aquí utilizaremos ./data/papers). Creamos un archivo arxiv_library.csv para almacenar las incrustaciones y los detalles de los artículos descargados para recuperarlos mediante summarize_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './data/papers' created successfully.\n"
     ]
    }
   ],
   "source": [
    "directory = './data/papers'\n",
    "\n",
    "if not os.path.exists(directory):    # Check if the directory already exists\n",
    "\n",
    "    # If the directory doesn't exist, create it and any necessary intermediate directories\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created successfully.\")\n",
    "else:\n",
    "\n",
    "    # If the directory already exists, print a message indicating it\n",
    "    print(f\"Directory '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory to store downloaded papers\n",
    "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
    "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3)) # Si la función falla, se reintenta con una espera que aumenta exponencialmente entre 1 y 40 segundos, hasta un máximo de 3 intentos.\n",
    "def embedding_request(text):\n",
    "    response = openai.embeddings.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "# def get_embedding(mi_texto, model= \"text-embedding-ada-002\"):\n",
    "#     text = mi_texto.replace('\\n', ' ')\n",
    "#     respuesta = openai.embeddings.create(input= text, model= model)\n",
    "#     return respuesta.data[0].embedding\n",
    "\n",
    "\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
    "    \"\"\"\n",
    "    This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the \n",
    "    read_article_and_summarize.\n",
    "\n",
    "    La función get_articles busca artículos científicos en ArXiv basados en una consulta \n",
    "    (query). Los resultados se limitan a los top_k artículos más relevantes. Guarda los \n",
    "    metadatos de estos artículos en un archivo CSV (library).\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(                                # Realiza una búsqueda en ArXiv utilizando los criterios dados.\n",
    "        query=query, \n",
    "        max_results=top_k, \n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    result_list = []\n",
    "    for result in search.results():                       # Itera sobre cada artículo encontrado en la búsqueda.\n",
    "        result_dict = {}                                  # Crea un diccionario con los metadatos del artículo, incluyendo el título, el resumen y las URLs del artículo y del PDF.\n",
    "        result_dict.update({\"title\"  : result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\"    : [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title) # Obtiene el embedding del título del artículo usando embedding_request y \n",
    "        file_reference = [                              # prepara una referencia de archivo, que incluye el título, la ruta de descarga del PDF, y el embedding.\n",
    "            result.title,\n",
    "            result.download_pdf(data_dir),\n",
    "            # response[\"data\"][0][\"embedding\"],\n",
    "            response.data[0].embedding\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:      # Abre el archivo CSV en modo de añadido (\"a\") y escribe la referencia del archivo (metadatos del artículo) en él.\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "            \n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\plane\\AppData\\Local\\Temp\\ipykernel_14232\\3255767048.py:21: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Proximal Policy Optimization and its Dynamic Version for Sequence Generation',\n",
       " 'summary': 'In sequence generation task, many works use policy gradient for model\\noptimization to tackle the intractable backpropagation issue when maximizing\\nthe non-differentiable evaluation metrics or fooling the discriminator in\\nadversarial learning. In this paper, we replace policy gradient with proximal\\npolicy optimization (PPO), which is a proved more efficient reinforcement\\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\\nperformance.',\n",
       " 'article_url': 'http://arxiv.org/abs/1808.07982v1',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/1808.07982v1'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the search is working\n",
    "result_output = get_articles(\"ppo reinforcement learning\")\n",
    "result_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title :  Proximal Policy Optimization and its Dynamic Version for Sequence Generation\n",
      "summary :  In sequence generation task, many works use policy gradient for model\n",
      "optimization to tackle the intractable backpropagation issue when maximizing\n",
      "the non-differentiable evaluation metrics or fooling the discriminator in\n",
      "adversarial learning. In this paper, we replace policy gradient with proximal\n",
      "policy optimization (PPO), which is a proved more efficient reinforcement\n",
      "learning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\n",
      "demonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\n",
      "generation tasks including synthetic experiment and chit-chat chatbot. The\n",
      "results show that PPO and PPO-dynamic can beat policy gradient by stability and\n",
      "performance.\n",
      "article_url :  http://arxiv.org/abs/1808.07982v1\n",
      "pdf_url :  http://arxiv.org/pdf/1808.07982v1\n",
      "\n",
      "\n",
      "title :  CIM-PPO:Proximal Policy Optimization with Liu-Correntropy Induced Metric\n",
      "summary :  As an algorithm based on deep reinforcement learning, Proximal Policy\n",
      "Optimization (PPO) performs well in many complex tasks and has become one of\n",
      "the most popular RL algorithms in recent years. According to the mechanism of\n",
      "penalty in surrogate objective, PPO can be divided into PPO with KL Divergence\n",
      "(KL-PPO) and PPO with Clip function(Clip-PPO). Clip-PPO is widely used in a\n",
      "variety of practical scenarios and has attracted the attention of many\n",
      "researchers. Therefore, many variations have also been created, making the\n",
      "algorithm better and better. However, as a more theoretical algorithm, KL-PPO\n",
      "was neglected because its performance was not as good as CliP-PPO. In this\n",
      "article, we analyze the asymmetry effect of KL divergence on PPO's objective\n",
      "function , and give the inequality that can indicate when the asymmetry will\n",
      "affect the efficiency of KL-PPO. Proposed PPO with Correntropy Induced Metric\n",
      "algorithm(CIM-PPO) that use the theory of correntropy(a symmetry metric method\n",
      "that was widely used in M-estimation to evaluate two distributions'\n",
      "difference)and applied it in PPO. Then, we designed experiments based on\n",
      "OpenAIgym to test the effectiveness of the new algorithm and compare it with\n",
      "KL-PPO and CliP-PPO.\n",
      "article_url :  http://arxiv.org/abs/2110.10522v2\n",
      "pdf_url :  http://arxiv.org/pdf/2110.10522v2\n",
      "\n",
      "\n",
      "title :  A2C is a special case of PPO\n",
      "summary :  Advantage Actor-critic (A2C) and Proximal Policy Optimization (PPO) are\n",
      "popular deep reinforcement learning algorithms used for game AI in recent\n",
      "years. A common understanding is that A2C and PPO are separate algorithms\n",
      "because PPO's clipped objective appears significantly different than A2C's\n",
      "objective. In this paper, however, we show A2C is a special case of PPO. We\n",
      "present theoretical justifications and pseudocode analysis to demonstrate why.\n",
      "To validate our claim, we conduct an empirical experiment using\n",
      "\\texttt{Stable-baselines3}, showing A2C and PPO produce the \\textit{exact} same\n",
      "models when other settings are controlled.\n",
      "article_url :  http://arxiv.org/abs/2205.09123v1\n",
      "pdf_url :  http://arxiv.org/pdf/2205.09123v1\n",
      "\n",
      "\n",
      "title :  Proximal Policy Optimization via Enhanced Exploration Efficiency\n",
      "summary :  Proximal policy optimization (PPO) algorithm is a deep reinforcement learning\n",
      "algorithm with outstanding performance, especially in continuous control tasks.\n",
      "But the performance of this method is still affected by its exploration\n",
      "ability. For classical reinforcement learning, there are some schemes that make\n",
      "exploration more full and balanced with data exploitation, but they can't be\n",
      "applied in complex environments due to the complexity of algorithm. Based on\n",
      "continuous control tasks with dense reward, this paper analyzes the assumption\n",
      "of the original Gaussian action exploration mechanism in PPO algorithm, and\n",
      "clarifies the influence of exploration ability on performance. Afterward,\n",
      "aiming at the problem of exploration, an exploration enhancement mechanism\n",
      "based on uncertainty estimation is designed in this paper. Then, we apply\n",
      "exploration enhancement theory to PPO algorithm and propose the proximal policy\n",
      "optimization algorithm with intrinsic exploration module (IEM-PPO) which can be\n",
      "used in complex environments. In the experimental parts, we evaluate our method\n",
      "on multiple tasks of MuJoCo physical simulator, and compare IEM-PPO algorithm\n",
      "with curiosity driven exploration algorithm (ICM-PPO) and original algorithm\n",
      "(PPO). The experimental results demonstrate that IEM-PPO algorithm needs longer\n",
      "training time, but performs better in terms of sample efficiency and cumulative\n",
      "reward, and has stability and robustness.\n",
      "article_url :  http://arxiv.org/abs/2011.05525v1\n",
      "pdf_url :  http://arxiv.org/pdf/2011.05525v1\n",
      "\n",
      "\n",
      "title :  Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks\n",
      "summary :  Most reinforcement learning methods rely heavily on dense, well-normalized\n",
      "environment rewards. DreamerV3 recently introduced a model-based method with a\n",
      "number of tricks that mitigate these limitations, achieving state-of-the-art on\n",
      "a wide range of benchmarks with a single set of hyperparameters. This result\n",
      "sparked discussion about the generality of the tricks, since they appear to be\n",
      "applicable to other reinforcement learning algorithms. Our work applies\n",
      "DreamerV3's tricks to PPO and is the first such empirical study outside of the\n",
      "original work. Surprisingly, we find that the tricks presented do not transfer\n",
      "as general improvements to PPO. We use a high quality PPO reference\n",
      "implementation and present extensive ablation studies totaling over 10,000 A100\n",
      "hours on the Arcade Learning Environment and the DeepMind Control Suite. Though\n",
      "our experiments demonstrate that these tricks do not generally outperform PPO,\n",
      "we identify cases where they succeed and offer insight into the relationship\n",
      "between the implementation tricks. In particular, PPO with these tricks\n",
      "performs comparably to PPO on Atari games with reward clipping and\n",
      "significantly outperforms PPO without reward clipping.\n",
      "article_url :  http://arxiv.org/abs/2310.17805v1\n",
      "pdf_url :  http://arxiv.org/pdf/2310.17805v1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in result_output:\n",
    "    for key, value in result.items():\n",
    "        print(key, ': ', value)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    ") -> list[str]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "\n",
    "    query_embedding_response = embedding_request(query)          # se obtiene el embedding de la consulta\n",
    "    query_embedding = query_embedding_response.data[0].embedding # query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    strings_and_relatednesses = [                                \n",
    "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Las rutas de archivos se ordenan en función de su similitud con la consulta y se devuelven las top_n rutas más relevantes.\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"\n",
    "    Takes a filepath to a PDF and returns a string of the PDF's contents\n",
    "    \n",
    "    Esta función toma una ruta de archivo PDF y devuelve su contenido como texto.\n",
    "    \"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:    # Itera sobre cada página del PDF, extrayendo el texto y anotando el número de página.\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns successive n-sized chunks from provided text.\n",
    "\n",
    "    Esta función divide un texto en segmentos (chunks) de \n",
    "    tamaño aproximado n, utilizando un tokenizador.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    # Se itera sobre los tokens del texto, buscando el final de las oraciones para dividir el texto de manera coherente.\n",
    "    while i < len(tokens):  \n",
    "\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            \n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"\n",
    "    This function applies a prompt to some input content. In this case it returns a summarized chunk of text\n",
    "\n",
    "    Aplica un prompt a un contenido de entrada y devuelve una parte resumida del texto.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.chat.completions.create(\n",
    "        model=GPT_MODEL, \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content #response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"\n",
    "    This function does the following:\n",
    "        - Reads in the arxiv_library.csv file in including the embeddings\n",
    "        - Finds the closest file to the user's query\n",
    "        - Scrapes the text out of the file and chunks it\n",
    "        - Summarizes each chunk in parallel\n",
    "        - Does one final summary and returns this to the user\n",
    "\n",
    "    Este bloque de código automatiza el proceso de resumir un documento académico \n",
    "    relacionado con una consulta del usuario. El proceso incluye:\n",
    "    \n",
    "        - la búsqueda del documento más relevante\n",
    "        - la extracción y fragmentación del texto. \n",
    "        - la generación de resúmenes utilizando la API de OpenAI.\n",
    "    \"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "\n",
    "    # Lee un archivo CSV (arxiv_library.csv) que contiene información sobre documentos académicos previamente descargados \n",
    "    # y sus embeddings. \n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "\n",
    "    # Si el archivo está vacío, realiza una búsqueda inicial para descargar documentos.\n",
    "    if len(library_df) == 0:           \n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "\n",
    "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)       # para encontrar el documento más relevante en relación con la consulta.\n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0])                                           # Lee el texto del documento \n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)                         # lo divide en fragmentos de tamaño manejable para el procesamiento\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    # Utiliza un ThreadPoolExecutor para procesar los resúmenes de cada \n",
    "    # fragmento de texto en paralelo, utilizando la función extract_chunk.\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=len(text_chunks)\n",
    "    ) as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    # Compila los resúmenes de los fragmentos y envía esta información a \n",
    "    # la API de OpenAI para generar un resumen final que responda a la consulta del usuario.\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = openai.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
    "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "                        User query: {query}\n",
    "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    }
   ],
   "source": [
    "# Test the summarize_text function works\n",
    "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Argument:\n",
      "The paper discusses the use of Proximal Policy Optimization (PPO) in sequence generation tasks, specifically in the context of chit-chat chatbots. The authors argue that PPO is a more efficient reinforcement learning algorithm compared to policy gradient, which is commonly used in these tasks. They propose a dynamic version of PPO (PPO-dynamic) and demonstrate its efficacy in synthetic experiments and chit-chat chatbot tasks.\n",
      "\n",
      "Evidence:\n",
      "- PPO-dynamic achieves high precision scores in a synthetic counting task, comparable to other algorithms such as REINFORCE and MIXER.\n",
      "- PPO-dynamic shows faster progress in learning compared to PPO in the synthetic counting task.\n",
      "- In the chit-chat chatbot task, PPO-dynamic achieves a slightly higher BLEU-2 score than REINFORCE and PPO.\n",
      "- The learning curves of PPO and PPO-dynamic are more stable than policy gradient, and PPO-dynamic converges faster.\n",
      "\n",
      "Conclusions:\n",
      "- PPO is a better optimization method for sequence learning compared to policy gradient.\n",
      "- PPO-dynamic further improves the optimization process by dynamically adjusting the hyperparameters.\n",
      "- PPO can be used as a new optimization method for GAN-based sequence learning for better performance.\n"
     ]
    }
   ],
   "source": [
    "print(chat_test_response.choices[0].message.content) # [\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Agent\n",
    "\n",
    "Crearemos nuestro agente en este paso, incluyendo una clase Conversation para soportar múltiples turnos con la API, y algunas funciones Python para permitir la interacción entre la <span style=\"color: rgb(0,255,0);\">***Chat Completions API***</span> y las funciones de nuestra base de conocimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"messages\": messages}\n",
    "    if functions is not None:\n",
    "        json_data.update({\"functions\": functions})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        self.conversation_history.append(message)\n",
    "\n",
    "    def display_conversation(self, detailed=False):\n",
    "        role_to_color = {\n",
    "            \"system\": \"red\",\n",
    "            \"user\": \"green\",\n",
    "            \"assistant\": \"blue\",\n",
    "            \"function\": \"magenta\",\n",
    "        }\n",
    "        for message in self.conversation_history:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
    "                    role_to_color[message[\"role\"]],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate our get_articles and read_article_and_summarize functions\n",
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"get_articles\",\n",
    "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
    "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            Description of the article in plain text based on the user's query\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
    "    response = chat_completion_request(messages, functions)\n",
    "    full_message = response.json()[\"choices\"][0]\n",
    "    if full_message[\"finish_reason\"] == \"function_call\":\n",
    "        print(f\"Function generation requested, calling function\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
    "    Currently extended by adding clauses to this if statement.\"\"\"\n",
    "\n",
    "    if full_message[\"message\"][\"function_call\"][\"name\"] == \"get_articles\":\n",
    "        try:\n",
    "            parsed_output = json.loads(\n",
    "                full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "            )\n",
    "            print(\"Getting search results\")\n",
    "            results = get_articles(parsed_output[\"query\"])\n",
    "        except Exception as e:\n",
    "            print(parsed_output)\n",
    "            print(f\"Function execution failed\")\n",
    "            print(f\"Error message: {e}\")\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": full_message[\"message\"][\"function_call\"][\"name\"],\n",
    "                \"content\": str(results),\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            print(\"Got search results, summarizing content\")\n",
    "            response = chat_completion_request(messages)\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(type(e))\n",
    "            raise Exception(\"Function chat request failed\")\n",
    "\n",
    "    elif (\n",
    "        full_message[\"message\"][\"function_call\"][\"name\"] == \"read_article_and_summarize\"\n",
    "    ):\n",
    "        parsed_output = json.loads(\n",
    "            full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "        )\n",
    "        print(\"Finding and reading paper\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Function does not exist and cannot be called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arXiv conversation\n",
    "\n",
    "Let's put this all together by testing our functions out in conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function\n",
      "Getting search results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\plane\\AppData\\Local\\Temp\\ipykernel_14232\\2972360465.py:20: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got search results, summarizing content\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "PPO (Proximal Policy Optimization) is an efficient reinforcement learning algorithm that has gained popularity in recent years. Here are a few papers that discuss PPO and its variants:\n",
       "\n",
       "1. Title: \"Proximal Policy Optimization and its Dynamic Version for Sequence Generation\"\n",
       "   - Summary: This paper explores the use of PPO for sequence generation tasks, such as chatbot development. The authors propose a dynamic approach called PPO-dynamic and show that PPO outperforms policy gradient in terms of stability and performance.\n",
       "   - Article URL: [arXiv:1808.07982v1](http://arxiv.org/abs/1808.07982v1)\n",
       "\n",
       "2. Title: \"CIM-PPO: Proximal Policy Optimization with Liu-Correntropy Induced Metric\"\n",
       "   - Summary: This article introduces a variation of PPO called CIM-PPO, which incorporates the theory of correntropy (a symmetry metric method) into PPO. The authors compare CIM-PPO with KL-PPO and Clip-PPO and conduct experiments on OpenAI gym to demonstrate the effectiveness of the new algorithm.\n",
       "   - Article URL: [arXiv:2110.10522v2](http://arxiv.org/abs/2110.10522v2)\n",
       "\n",
       "3. Title: \"A2C is a special case of PPO\"\n",
       "   - Summary: This paper presents theoretical justifications and pseudocode analysis to show that Advantage Actor-critic (A2C) is a special case of PPO. The authors provide empirical evidence using Stable-baselines3 to demonstrate that A2C and PPO produce the exact same models under controlled settings.\n",
       "   - Article URL: [arXiv:2205.09123v1](http://arxiv.org/abs/2205.09123v1)\n",
       "\n",
       "4. Title: \"Proximal Policy Optimization via Enhanced Exploration Efficiency\"\n",
       "   - Summary: This paper focuses on the exploration ability of PPO and proposes an exploration enhancement mechanism based on uncertainty estimation called IEM-PPO. The authors evaluate the method on MuJoCo physical simulator tasks and compare it with curiosity-driven exploration algorithms and the original PPO. The results show improved sample efficiency and cumulative reward with stability and robustness.\n",
       "   - Article URL: [arXiv:2011.05525v1](http://arxiv.org/abs/2011.05525v1)\n",
       "\n",
       "5. Title: \"Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks\"\n",
       "   - Summary: This work applies the tricks introduced in DreamerV3 (a model-based approach) to PPO. The study investigates the generality of these tricks and their applicability to PPO. The experiments conducted on the Arcade Learning Environment and the DeepMind Control Suite show that the tricks do not generally improve PPO's performance but highlight specific cases where they succeed.\n",
       "   - Article URL: [arXiv:2310.17805v1](http://arxiv.org/abs/2310.17805v1)\n",
       "\n",
       "You can access the full papers by clicking on the article URLs provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, how does PPO reinforcement learning work?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function\n",
      "Finding and reading paper\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:09<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Core Argument:\n",
       "- The paper discusses the use of proximal policy optimization (PPO) in sequence generation tasks, specifically in the context of chit-chat chatbots.\n",
       "- The authors argue that PPO is a more efficient reinforcement learning algorithm compared to policy gradient, which is commonly used in these tasks.\n",
       "- They propose a dynamic approach for PPO (PPO-dynamic) and demonstrate its efficacy in synthetic experiments and chit-chat chatbot tasks.\n",
       "\n",
       "Evidence:\n",
       "- PPO-dynamic achieves a high precision score in a synthetic counting task, comparable to other algorithms like REINFORCE and MIXER.\n",
       "- In the chit-chat chatbot task, PPO-dynamic achieves a slightly higher BLEU-2 score than REINFORCE and PPO.\n",
       "- The learning curves of PPO and PPO-dynamic are more stable than policy gradient, and PPO-dynamic converges faster.\n",
       "\n",
       "Conclusions:\n",
       "- PPO is a better optimization method for sequence learning compared to policy gradient.\n",
       "- PPO-dynamic further improves the optimization process by dynamically adjusting the hyperparameters.\n",
       "- PPO can be used as a new optimization method for GAN-based sequence learning for better performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Can you read the PPO sequence generation paper for me and give me a summary\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "display(Markdown(updated_response.choices[0].message.content))  #[\"choices\"][0][\"message\"][\"content\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
